{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки, которые будут использоваться в проекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dmitrii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dmitrii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Dmitrii\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dmitrii\\AppData\\Local\\Temp\\ipykernel_4732\\1542924373.py:12: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as stopwords_nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и изучение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем данные из исходной таблицы, смотрим общую информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments = pd.read_csv('datasets/toxic_comments.csv')\n",
    "\n",
    "display(comments.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропущенных значений нет, преобразование типов данных не требуется.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим примеры нескольких строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the tools well.  · talk \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27                                                                                                                                                                                                                                                                                                                                                                            \n",
       "1  D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "2  Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4  You, sir, are my hero. Any chance you remember what page that's on?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "5  \"\\n\\nCongratulations from me as well, use the tools well.  · talk \"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "6  COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "7  Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "8  Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169                                                                                                                                                             \n",
       "9  alignment on this subject and which are contrary to those of DuLithgow                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "   toxic  \n",
       "0  0      \n",
       "1  0      \n",
       "2  0      \n",
       "3  0      \n",
       "4  0      \n",
       "5  0      \n",
       "6  1      \n",
       "7  0      \n",
       "8  0      \n",
       "9  0      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(comments.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Негативные (токсичные) комментарии обозначены как *1* в столбце *toxic*.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на соотношение позитивных/нейтральных комментариев и негативных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "comments['toxic'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соотношение между классами примерно 90% к 10%, необходимо будет учесть данный дисбаланс при обучении моделей. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём обработку текстов, необходимую для того чтобы на них можно было провести обучение моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем лишние символы и переводим всё в нижний регистр\n",
    "comments['w_o_punct'] = comments['text'].apply(lambda x: re.sub(r'[^a-zA-Z ]', ' ', x).lower())\n",
    "\n",
    "# удаляем стоп-слова \n",
    "stop_words = set(stopwords_nltk.words('english')) \n",
    "comments['w_o_stopwords'] = comments['w_o_punct'].apply(lambda x: [w for w in x.split() if not w in stop_words])\n",
    "\n",
    "# лемматизация и перевод в unicode\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "comments['lemmas'] = comments['w_o_stopwords'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "comments['lemmas'] = comments['lemmas'].astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим результат проведённых операций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>w_o_punct</th>\n",
       "      <th>w_o_stopwords</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my username hardcore metallica fan were reverted  they weren t vandalisms  just closure on some gas after i voted at new york dolls fac  and please don t remove the template from the talk page since i m retired now</td>\n",
       "      <td>[explanation, edits, made, username, hardcore, metallica, fan, reverted, vandalisms, closure, gas, voted, new, york, dolls, fac, please, remove, template, talk, page, since, retired]</td>\n",
       "      <td>['explanation', 'edits', 'made', 'username', 'hardcore', 'metallica', 'fan', 'reverted', 'vandalism', 'closure', 'gas', 'voted', 'new', 'york', 'doll', 'fac', 'please', 'remove', 'template', 'talk', 'page', 'since', 'retired']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "      <td>d aww  he matches this background colour i m seemingly stuck with  thanks    talk         january           utc</td>\n",
       "      <td>[aww, matches, background, colour, seemingly, stuck, thanks, talk, january, utc]</td>\n",
       "      <td>['aww', 'match', 'background', 'colour', 'seemingly', 'stuck', 'thanks', 'talk', 'january', 'utc']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man  i m really not trying to edit war  it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page  he seems to care more about the formatting than the actual info</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, constantly, removing, relevant, information, talking, edits, instead, talk, page, seems, care, formatting, actual, info]</td>\n",
       "      <td>['hey', 'man', 'really', 'trying', 'edit', 'war', 'guy', 'constantly', 'removing', 'relevant', 'information', 'talking', 'edits', 'instead', 'talk', 'page', 'seems', 'care', 'formatting', 'actual', 'info']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                        text  \\\n",
       "0  Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1  D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)                                                                                                                                                            \n",
       "2  Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.                                   \n",
       "\n",
       "   toxic  \\\n",
       "0  0       \n",
       "1  0       \n",
       "2  0       \n",
       "\n",
       "                                                                                                                                                                                                                                                                  w_o_punct  \\\n",
       "0  explanation why the edits made under my username hardcore metallica fan were reverted  they weren t vandalisms  just closure on some gas after i voted at new york dolls fac  and please don t remove the template from the talk page since i m retired now                \n",
       "1  d aww  he matches this background colour i m seemingly stuck with  thanks    talk         january           utc                                                                                                                                                            \n",
       "2  hey man  i m really not trying to edit war  it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page  he seems to care more about the formatting than the actual info                                   \n",
       "\n",
       "                                                                                                                                                                            w_o_stopwords  \\\n",
       "0  [explanation, edits, made, username, hardcore, metallica, fan, reverted, vandalisms, closure, gas, voted, new, york, dolls, fac, please, remove, template, talk, page, since, retired]   \n",
       "1  [aww, matches, background, colour, seemingly, stuck, thanks, talk, january, utc]                                                                                                         \n",
       "2  [hey, man, really, trying, edit, war, guy, constantly, removing, relevant, information, talking, edits, instead, talk, page, seems, care, formatting, actual, info]                      \n",
       "\n",
       "                                                                                                                                                                                                                               lemmas  \n",
       "0  ['explanation', 'edits', 'made', 'username', 'hardcore', 'metallica', 'fan', 'reverted', 'vandalism', 'closure', 'gas', 'voted', 'new', 'york', 'doll', 'fac', 'please', 'remove', 'template', 'talk', 'page', 'since', 'retired']  \n",
       "1  ['aww', 'match', 'background', 'colour', 'seemingly', 'stuck', 'thanks', 'talk', 'january', 'utc']                                                                                                                                  \n",
       "2  ['hey', 'man', 'really', 'trying', 'edit', 'war', 'guy', 'constantly', 'removing', 'relevant', 'information', 'talking', 'edits', 'instead', 'talk', 'page', 'seems', 'care', 'formatting', 'actual', 'info']                       "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По результатам обработки текстов получаем в каждой строке набор лемм, на базе которых уже можно будет сформировать признаки для обучения моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка выборок для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Целевым признаком для обучения моделей будет являться токсичность комментариев из столбца *toxic*.\n",
    "\n",
    "Признаки для обучения будут сформированы как частота встречающихся слов, определенная с помощью TF-IDF ниже по столбцу *lemmas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = comments['toxic']\n",
    "features = comments['lemmas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделяем данные на обучающую, валидационную и тестовую выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features, target, train_size=0.8, test_size=0.2, random_state=12345, stratify=target)\n",
    "\n",
    "features_valid, features_test, target_valid, target_test = train_test_split(\n",
    "    features_valid, target_valid, train_size=0.5, test_size=0.5, random_state=12345, stratify=target_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Определяем частоту слов с помощью TF-IDF.\n",
    "\n",
    "Обучение для определения частоты слов проводим только на признаках из обучающей выборки, а преобразуем все выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# на базе обучающей выборки определяем список слов, встречающихся во всех её текстах\n",
    "count_tf_idf = TfidfVectorizer(stop_words=stop_words)\n",
    "tf_idf = count_tf_idf.fit(features_train)\n",
    "\n",
    "# преобразуем признак с набором лемм в набор признаков-слов с частотой их использования в каждой строке для обеих выборок\n",
    "features_train = tf_idf.transform(features_train)\n",
    "features_valid = tf_idf.transform(features_valid)\n",
    "features_test = tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Убеждаемся в корректном разделении данных на выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер таблицы с признаками в обучающей выборке (127656, 138543)\n",
      "Размер таблицы с целевым признаком в обучающей выборке (127656,)\n",
      "Размер таблицы с признаками в валидационной выборке (15957, 138543)\n",
      "Размер таблицы с целевым признаком в валидационной выборке (15957,)\n",
      "Размер таблицы с признаками в тестовой выборке (15958, 138543)\n",
      "Размер таблицы с целевым признаком в тестовой выборке (15958,)\n"
     ]
    }
   ],
   "source": [
    "print('Размер таблицы с признаками в обучающей выборке', features_train.shape)\n",
    "print('Размер таблицы с целевым признаком в обучающей выборке', target_train.shape)\n",
    "print('Размер таблицы с признаками в валидационной выборке', features_valid.shape)\n",
    "print('Размер таблицы с целевым признаком в валидационной выборке', target_valid.shape)\n",
    "print('Размер таблицы с признаками в тестовой выборке', features_test.shape)\n",
    "print('Размер таблицы с целевым признаком в тестовой выборке', target_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные подготовлены к поиску наилучшей модели.\n",
    "\n",
    "Определена частота, с которой каждое из более чем 100 тыс. слов появляется в каждом тексте."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём обучение модели Логистической регрессии на обучающей выборке, предскажем негативность комментариев для тестовой выборки и вычислим метрику F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Dmitrii\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Dmitrii\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 18s\n",
      "Wall time: 34.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'sag'}</td>\n",
       "      <td>0.762555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'newton-cg'}</td>\n",
       "      <td>0.762347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}</td>\n",
       "      <td>0.762139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}</td>\n",
       "      <td>0.762139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>{'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}</td>\n",
       "      <td>0.761101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Algorithm  \\\n",
       "1  LogisticRegression   \n",
       "0  LogisticRegression   \n",
       "3  LogisticRegression   \n",
       "4  LogisticRegression   \n",
       "2  LogisticRegression   \n",
       "\n",
       "                                                        Hyperparameters  \\\n",
       "1  {'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'sag'}         \n",
       "0  {'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'newton-cg'}   \n",
       "3  {'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'lbfgs'}       \n",
       "4  {'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'liblinear'}   \n",
       "2  {'class_weight': 'balanced', 'penalty': 'l2', 'solver': 'saga'}        \n",
       "\n",
       "   F1 score  \n",
       "1  0.762555  \n",
       "0  0.762347  \n",
       "3  0.762139  \n",
       "4  0.762139  \n",
       "2  0.761101  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "f1_lr=[]\n",
    "\n",
    "for solver in ['newton-cg', 'sag', 'saga', 'lbfgs', 'liblinear']:\n",
    "    model = LogisticRegression(random_state=12345, class_weight='balanced', penalty='l2', solver=solver) \n",
    "    model.fit(features_train, target_train)\n",
    "    predictions = model.predict(features_valid)\n",
    "    f1 = f1_score(target_valid, predictions)\n",
    "    f1_lr.append({'Algorithm': 'LogisticRegression', \n",
    "                  'Hyperparameters': {'class_weight':'balanced', 'penalty':'l2', 'solver': solver}, \n",
    "                  'F1 score': f1})\n",
    "        \n",
    "pd.DataFrame(f1_lr).sort_values('F1 score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью добавления гиперпараметров, в частности *penalty='l2'*, удалось достигнуть требуемый уровень качества, при котором значение метрики F1 выше 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём обучение модели с помощью алгоритма классификации Случайный лес на обучающей выборке, предскажем негативность комментариев для тестовой выборки и вычислим метрику F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 26s\n",
      "Wall time: 3min 27s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 70}</td>\n",
       "      <td>0.510127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 70}</td>\n",
       "      <td>0.509322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 70}</td>\n",
       "      <td>0.506667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 60}</td>\n",
       "      <td>0.493935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 60}</td>\n",
       "      <td>0.489126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 60}</td>\n",
       "      <td>0.474091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'n_estimators': 30, 'max_depth': 50}</td>\n",
       "      <td>0.467523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'n_estimators': 20, 'max_depth': 50}</td>\n",
       "      <td>0.463908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>{'n_estimators': 40, 'max_depth': 50}</td>\n",
       "      <td>0.463618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Algorithm                        Hyperparameters  F1 score\n",
       "8  RandomForestClassifier  {'n_estimators': 40, 'max_depth': 70}  0.510127\n",
       "2  RandomForestClassifier  {'n_estimators': 20, 'max_depth': 70}  0.509322\n",
       "5  RandomForestClassifier  {'n_estimators': 30, 'max_depth': 70}  0.506667\n",
       "7  RandomForestClassifier  {'n_estimators': 40, 'max_depth': 60}  0.493935\n",
       "4  RandomForestClassifier  {'n_estimators': 30, 'max_depth': 60}  0.489126\n",
       "1  RandomForestClassifier  {'n_estimators': 20, 'max_depth': 60}  0.474091\n",
       "3  RandomForestClassifier  {'n_estimators': 30, 'max_depth': 50}  0.467523\n",
       "0  RandomForestClassifier  {'n_estimators': 20, 'max_depth': 50}  0.463908\n",
       "6  RandomForestClassifier  {'n_estimators': 40, 'max_depth': 50}  0.463618"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "f1_rfc = []\n",
    "\n",
    "for est in range(20, 41, 10):\n",
    "    for depth in range(50, 71, 10):            \n",
    "        model = RandomForestClassifier(random_state=12345, class_weight = 'balanced', \n",
    "                                       n_estimators=est, max_depth=depth)        \n",
    "        model.fit(features_train, target_train)\n",
    "        predictions = model.predict(features_valid)\n",
    "        f1 = f1_score(target_valid, predictions)\n",
    "        f1_rfc.append({'Algorithm': 'RandomForestClassifier', \n",
    "                       'Hyperparameters': {'n_estimators': est, 'max_depth': depth}, \n",
    "                       'F1 score': f1})\n",
    "        \n",
    "pd.DataFrame(f1_rfc).sort_values('F1 score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат работы Случайного леса существенно ниже требуемых значений по метрике F1, хуже Логистической регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём обучение модели с помощью алгоритма классификации на базе градиентного бустинга LightGBM на обучающей выборке, предскажем негативность комментариев для тестовой выборки и вычислим метрику F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 47min 36s\n",
      "Wall time: 8min 10s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Hyperparameters</th>\n",
       "      <th>F1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.25}</td>\n",
       "      <td>0.779376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.5}</td>\n",
       "      <td>0.779089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.5}</td>\n",
       "      <td>0.777589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.5}</td>\n",
       "      <td>0.776876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.25}</td>\n",
       "      <td>0.772375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.25}</td>\n",
       "      <td>0.771843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.25}</td>\n",
       "      <td>0.771642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.5}</td>\n",
       "      <td>0.771352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.5}</td>\n",
       "      <td>0.768814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.25}</td>\n",
       "      <td>0.767094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.5}</td>\n",
       "      <td>0.760218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LGBMClassifier</td>\n",
       "      <td>{'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.25}</td>\n",
       "      <td>0.759019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Algorithm  \\\n",
       "10  LGBMClassifier   \n",
       "11  LGBMClassifier   \n",
       "3   LGBMClassifier   \n",
       "7   LGBMClassifier   \n",
       "8   LGBMClassifier   \n",
       "6   LGBMClassifier   \n",
       "2   LGBMClassifier   \n",
       "9   LGBMClassifier   \n",
       "5   LGBMClassifier   \n",
       "4   LGBMClassifier   \n",
       "1   LGBMClassifier   \n",
       "0   LGBMClassifier   \n",
       "\n",
       "                                                 Hyperparameters  F1 score  \n",
       "10  {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.25}  0.779376  \n",
       "11  {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.5}   0.779089  \n",
       "3   {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.5}   0.777589  \n",
       "7   {'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.5}   0.776876  \n",
       "8   {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.25}  0.772375  \n",
       "6   {'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.25}  0.771843  \n",
       "2   {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.25}  0.771642  \n",
       "9   {'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.5}   0.771352  \n",
       "5   {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.5}   0.768814  \n",
       "4   {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.25}  0.767094  \n",
       "1   {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.5}   0.760218  \n",
       "0   {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.25}  0.759019  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "f1_lgbm = []\n",
    "\n",
    "for est in range(300, 501, 100):\n",
    "    for depth in range (4, 6, 1):\n",
    "        for lr in [0.25, 0.5]:\n",
    "            model = lgb.LGBMClassifier(n_estimators=est, learning_rate=lr, max_depth=depth, random_state=12345)\n",
    "            model.fit(features_train, target_train)\n",
    "            predictions = model.predict(features_valid)\n",
    "            f1 = f1_score(target_valid, predictions)\n",
    "            f1_lgbm.append({'Algorithm': 'LGBMClassifier',\n",
    "                            'Hyperparameters': {'n_estimators': est, 'max_depth': depth, 'learning_rate': lr},\n",
    "                            'F1 score': f1})\n",
    "\n",
    "pd.DataFrame(f1_lgbm).sort_values('F1 score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор на базе градиентного бустинга LightGBM с высокими значениями гиперпараметров позволяет достичь требуемых значений метрики F1 в 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках обучения и валидации наилучшие результаты по метрике F1 показала сделующая модель:\n",
    "* Алгоритм: LGBMClassifier\n",
    "* Гиперпараметры:\n",
    "    * random_state=12345,\n",
    "    * n_estimators=500,\n",
    "    * max_depth=5,\n",
    "    * learning_rate= 0.25\n",
    "\n",
    "Проверим её эффективность на тестовой выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score наилучшей модели на тестовой выборке 0.7510608203677511\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(n_estimators=500, learning_rate=0.25, max_depth=5, random_state=12345) \n",
    "model.fit(features_train, target_train)\n",
    "predictions = model.predict(features_test)\n",
    "f1 = f1_score(target_test, predictions)\n",
    "print('F1 Score наилучшей модели на тестовой выборке', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках проекта была определена наилучшая модель, которая позволяет определять негативность (токсичность) комментариев пользователей с требуемым уровнем качества. \n",
    "\n",
    "Значение метрики F1 данной модели на тестовой выборке составило ~0.7544, что выше требуемых целями проекта 0.75.\n",
    "\n",
    "В основе данной модели - классификатор LGBMClassifier с гиперпараметрами: \n",
    "* random_state=12345, \n",
    "* n_estimators=500, \n",
    "* max_depth=5, \n",
    "* learning_rate= 0.25.\n",
    "\n",
    "В ходе выполнения работ:\n",
    "* Импортирован датасет с размеченными данными по негативности комментариев.\n",
    "* Преобразован исходный текст комментариев:\n",
    "    * удалены лишние символы\n",
    "    * всё переведено в нижний регистр\n",
    "    * удалены стоп-слова\n",
    "    * проведена лемматизация\n",
    "    * всё переведено в unicode\n",
    "* Сформированы обучающая и тестовая выборки, признаками которых стали частоты, с которыми в каждом тексте встречаются слова (леммы) из всего датасета.\n",
    "* Проведено обучение моделей на основе 3 алгоритмов, один из которых обеспечил требуемый результат."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 9552,
    "start_time": "2021-06-08T19:54:25.306Z"
   },
   {
    "duration": 1166,
    "start_time": "2021-06-08T20:05:21.143Z"
   },
   {
    "duration": 16,
    "start_time": "2021-06-08T20:05:31.046Z"
   },
   {
    "duration": 35,
    "start_time": "2021-06-08T20:23:20.649Z"
   },
   {
    "duration": 594,
    "start_time": "2021-06-08T20:23:25.022Z"
   },
   {
    "duration": 346,
    "start_time": "2021-06-08T20:24:25.391Z"
   },
   {
    "duration": 8,
    "start_time": "2021-06-08T21:03:08.862Z"
   },
   {
    "duration": 123,
    "start_time": "2021-06-08T21:05:10.253Z"
   },
   {
    "duration": 1727,
    "start_time": "2021-06-08T21:05:18.922Z"
   },
   {
    "duration": 20,
    "start_time": "2021-06-08T21:05:24.159Z"
   },
   {
    "duration": 12,
    "start_time": "2021-06-08T21:09:27.730Z"
   },
   {
    "duration": 119,
    "start_time": "2021-06-08T21:09:58.916Z"
   },
   {
    "duration": 2378,
    "start_time": "2021-06-08T21:10:05.497Z"
   },
   {
    "duration": 24011,
    "start_time": "2021-06-08T21:10:18.087Z"
   },
   {
    "duration": 2145,
    "start_time": "2021-06-08T21:11:01.110Z"
   },
   {
    "duration": 1830,
    "start_time": "2021-06-08T21:15:43.660Z"
   },
   {
    "duration": 5169,
    "start_time": "2021-06-08T21:21:09.096Z"
   },
   {
    "duration": 5624,
    "start_time": "2021-06-08T21:23:20.344Z"
   },
   {
    "duration": 40451,
    "start_time": "2021-06-08T21:23:34.336Z"
   },
   {
    "duration": 8,
    "start_time": "2021-06-08T21:30:51.895Z"
   },
   {
    "duration": 38717,
    "start_time": "2021-06-08T21:30:54.656Z"
   },
   {
    "duration": 38780,
    "start_time": "2021-06-08T21:32:40.124Z"
   },
   {
    "duration": 149,
    "start_time": "2021-06-08T21:54:46.511Z"
   },
   {
    "duration": 1079,
    "start_time": "2021-06-08T21:54:51.631Z"
   },
   {
    "duration": 7573,
    "start_time": "2021-06-08T21:54:55.613Z"
   },
   {
    "duration": 7,
    "start_time": "2021-06-08T21:55:12.702Z"
   },
   {
    "duration": 8735,
    "start_time": "2021-06-08T21:55:52.453Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-08T21:56:08.270Z"
   },
   {
    "duration": 5,
    "start_time": "2021-06-08T21:57:01.176Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-08T21:57:35.316Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-08T21:57:44.125Z"
   },
   {
    "duration": 18259,
    "start_time": "2021-06-08T22:16:40.540Z"
   },
   {
    "duration": 5,
    "start_time": "2021-06-08T22:17:44.815Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-08T22:18:00.475Z"
   },
   {
    "duration": 15154,
    "start_time": "2021-06-08T22:20:28.488Z"
   },
   {
    "duration": 16617,
    "start_time": "2021-06-08T22:21:56.838Z"
   },
   {
    "duration": 14792,
    "start_time": "2021-06-08T22:22:16.690Z"
   },
   {
    "duration": 129512,
    "start_time": "2021-06-08T22:28:13.806Z"
   },
   {
    "duration": 9928,
    "start_time": "2021-06-08T22:31:22.841Z"
   },
   {
    "duration": 433583,
    "start_time": "2021-06-08T22:33:07.493Z"
   },
   {
    "duration": 534023,
    "start_time": "2021-06-08T22:41:35.392Z"
   },
   {
    "duration": 371,
    "start_time": "2021-06-08T22:50:29.417Z"
   },
   {
    "duration": 5,
    "start_time": "2021-06-08T22:50:35.096Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-08T22:51:42.591Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-08T22:52:33.208Z"
   },
   {
    "duration": 5,
    "start_time": "2021-06-08T22:52:54.798Z"
   },
   {
    "duration": 2965,
    "start_time": "2021-06-08T23:07:49.123Z"
   },
   {
    "duration": 909,
    "start_time": "2021-06-08T23:07:52.091Z"
   },
   {
    "duration": 17,
    "start_time": "2021-06-08T23:07:53.002Z"
   },
   {
    "duration": 265,
    "start_time": "2021-06-08T23:07:53.022Z"
   },
   {
    "duration": 43806,
    "start_time": "2021-06-08T23:07:53.289Z"
   },
   {
    "duration": 19,
    "start_time": "2021-06-08T23:08:37.097Z"
   },
   {
    "duration": 11,
    "start_time": "2021-06-08T23:08:37.119Z"
   },
   {
    "duration": 51,
    "start_time": "2021-06-08T23:08:37.133Z"
   },
   {
    "duration": 15977,
    "start_time": "2021-06-08T23:08:37.209Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-08T23:08:53.188Z"
   },
   {
    "duration": 14436,
    "start_time": "2021-06-08T23:09:25.176Z"
   },
   {
    "duration": 74345,
    "start_time": "2021-06-08T23:09:53.664Z"
   },
   {
    "duration": 44893,
    "start_time": "2021-06-08T23:15:48.145Z"
   },
   {
    "duration": 138791,
    "start_time": "2021-06-08T23:17:36.860Z"
   },
   {
    "duration": 322337,
    "start_time": "2021-06-08T23:20:32.257Z"
   },
   {
    "duration": 238016,
    "start_time": "2021-06-08T23:28:54.672Z"
   },
   {
    "duration": 448076,
    "start_time": "2021-06-08T23:39:02.298Z"
   },
   {
    "duration": 5183760,
    "start_time": "2021-06-09T00:07:16.805Z"
   },
   {
    "duration": 3215,
    "start_time": "2021-06-09T07:59:24.643Z"
   },
   {
    "duration": 805,
    "start_time": "2021-06-09T07:59:27.861Z"
   },
   {
    "duration": 14,
    "start_time": "2021-06-09T07:59:28.669Z"
   },
   {
    "duration": 380,
    "start_time": "2021-06-09T07:59:28.687Z"
   },
   {
    "duration": 40368,
    "start_time": "2021-06-09T07:59:29.069Z"
   },
   {
    "duration": 19,
    "start_time": "2021-06-09T08:00:09.440Z"
   },
   {
    "duration": 37,
    "start_time": "2021-06-09T08:00:09.462Z"
   },
   {
    "duration": 57,
    "start_time": "2021-06-09T08:00:09.502Z"
   },
   {
    "duration": 15162,
    "start_time": "2021-06-09T08:00:09.562Z"
   },
   {
    "duration": 5,
    "start_time": "2021-06-09T08:00:24.726Z"
   },
   {
    "duration": 14682,
    "start_time": "2021-06-09T08:01:08.477Z"
   },
   {
    "duration": 1552,
    "start_time": "2021-06-09T08:32:02.272Z"
   },
   {
    "duration": 797,
    "start_time": "2021-06-09T08:32:03.827Z"
   },
   {
    "duration": 11,
    "start_time": "2021-06-09T08:32:04.627Z"
   },
   {
    "duration": 322,
    "start_time": "2021-06-09T08:32:04.655Z"
   },
   {
    "duration": 40035,
    "start_time": "2021-06-09T08:32:04.980Z"
   },
   {
    "duration": 15,
    "start_time": "2021-06-09T08:32:45.017Z"
   },
   {
    "duration": 19,
    "start_time": "2021-06-09T08:32:45.035Z"
   },
   {
    "duration": 34,
    "start_time": "2021-06-09T08:32:45.057Z"
   },
   {
    "duration": 14994,
    "start_time": "2021-06-09T08:32:45.094Z"
   },
   {
    "duration": 7,
    "start_time": "2021-06-09T08:33:00.090Z"
   },
   {
    "duration": 14457,
    "start_time": "2021-06-09T08:33:00.100Z"
   },
   {
    "duration": 85502,
    "start_time": "2021-06-09T08:33:14.559Z"
   },
   {
    "duration": 1512,
    "start_time": "2021-06-09T08:35:31.305Z"
   },
   {
    "duration": 801,
    "start_time": "2021-06-09T08:35:32.820Z"
   },
   {
    "duration": 13,
    "start_time": "2021-06-09T08:35:33.624Z"
   },
   {
    "duration": 338,
    "start_time": "2021-06-09T08:35:33.655Z"
   },
   {
    "duration": 41168,
    "start_time": "2021-06-09T08:35:33.996Z"
   },
   {
    "duration": 15,
    "start_time": "2021-06-09T08:36:15.167Z"
   },
   {
    "duration": 24,
    "start_time": "2021-06-09T08:36:15.184Z"
   },
   {
    "duration": 42,
    "start_time": "2021-06-09T08:36:15.212Z"
   },
   {
    "duration": 14837,
    "start_time": "2021-06-09T08:36:15.257Z"
   },
   {
    "duration": 6,
    "start_time": "2021-06-09T08:36:30.096Z"
   },
   {
    "duration": 14549,
    "start_time": "2021-06-09T08:36:30.105Z"
   },
   {
    "duration": 248036,
    "start_time": "2021-06-09T08:36:44.657Z"
   },
   {
    "duration": 6063264,
    "start_time": "2021-06-09T08:40:52.696Z"
   },
   {
    "duration": 1207,
    "start_time": "2021-06-09T19:32:23.265Z"
   },
   {
    "duration": 3147,
    "start_time": "2021-06-09T19:32:24.474Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-09T19:32:27.624Z"
   },
   {
    "duration": 210,
    "start_time": "2021-06-09T19:32:27.636Z"
   },
   {
    "duration": 22356,
    "start_time": "2021-06-09T19:32:27.848Z"
   },
   {
    "duration": 9,
    "start_time": "2021-06-09T19:32:50.205Z"
   },
   {
    "duration": 17,
    "start_time": "2021-06-09T19:32:50.217Z"
   },
   {
    "duration": 101,
    "start_time": "2021-06-09T19:32:50.237Z"
   },
   {
    "duration": 8308,
    "start_time": "2021-06-09T19:34:51.288Z"
   },
   {
    "duration": 7,
    "start_time": "2021-06-09T19:35:37.864Z"
   },
   {
    "duration": 11968,
    "start_time": "2021-06-09T19:36:28.371Z"
   },
   {
    "duration": 47859,
    "start_time": "2021-06-09T19:52:35.276Z"
   },
   {
    "duration": 339,
    "start_time": "2021-06-09T19:53:40.582Z"
   },
   {
    "duration": 63,
    "start_time": "2021-06-09T19:55:45.833Z"
   },
   {
    "duration": 190518,
    "start_time": "2021-06-09T19:55:56.534Z"
   },
   {
    "duration": 111,
    "start_time": "2021-06-09T20:00:27.437Z"
   },
   {
    "duration": 9164,
    "start_time": "2021-06-09T20:00:30.520Z"
   },
   {
    "duration": 4,
    "start_time": "2021-06-09T20:00:45.081Z"
   },
   {
    "duration": 157260,
    "start_time": "2021-06-09T20:03:39.010Z"
   },
   {
    "duration": 132,
    "start_time": "2021-06-09T20:06:20.205Z"
   },
   {
    "duration": 60,
    "start_time": "2021-06-09T20:07:01.339Z"
   },
   {
    "duration": 122,
    "start_time": "2021-06-09T20:07:10.866Z"
   },
   {
    "duration": 105609,
    "start_time": "2021-06-09T20:07:34.941Z"
   },
   {
    "duration": 10,
    "start_time": "2021-06-09T20:12:46.984Z"
   },
   {
    "duration": 11,
    "start_time": "2021-06-09T20:13:36.097Z"
   },
   {
    "duration": 105328,
    "start_time": "2021-06-09T20:14:18.530Z"
   },
   {
    "duration": 4133,
    "start_time": "2021-06-09T20:17:59.947Z"
   },
   {
    "duration": 151353,
    "start_time": "2021-06-09T20:23:22.740Z"
   },
   {
    "duration": 1636666,
    "start_time": "2021-06-09T20:29:47.156Z"
   },
   {
    "duration": 693083,
    "start_time": "2021-06-09T21:01:49.812Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "db4e45e668acaf9ffaf99cd3276233483bc49e1698a7166bd9a998094d426896"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
